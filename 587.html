
<html>
<head>
  <title>CSE 587 - Penn State University</title>
  <link rel="stylesheet" type="text/css" href="courses.css"/>
</head>

<body>

<h1 style="color:#002D62">CSE 587 Spring 2023: Deep Learning for Natural Language Processing</h1>



<h2>Course Information</h2>
Instructor
    <ul>
        <li><a href="https://ryanzhumich.github.io/">Rui Zhang</a></li>
        <li>Office Hour: Wednesdays 4-6pm @ W329 Westgate Building and Zoom</li>
    </ul>
Lecture Time and Location
    <ul>
        <li>Mondays and Wednesdays 2:30pm - 3:45pm @ Walker Building 124</li>
    </ul>
TA
    <ul>
        <li>Rishabh Bhatt</li>
        <li>Office Hour: Thursdays 12-2pm @ W300</li>
    </ul>
Course Syllabus
    <ul>
        <li>Detailed syllabus is avaialbe <a href="https://docs.google.com/document/d/1vurzuT5vF-d4MIbegKFWeVw50R-20OoCO7QUmJHSKM0/edit?usp=sharing">here</a>.</li>
    </ul>
    

<h2>Course Goals and Objectives</h2>
Students will gain necessary skills and experience to understand, design, implement, and test their own NLP models using neural networks through programming assignments and a final project.  After successfully completing this course, students will be able to:
    <ol>
        <li>Design, implement, and test NLP models based neural networks</li>
        <li>Analyze and assess the performance of NLP models</li>
        <li>Situate their research contributions with reference to the state-of-the-art</li>
        <li>Present their results in an academic fashion including both research papers and oral presentations</li>
    </ol>

<h2>Prerequisites</h2>
    Since this course centers on deep learning methodology for NLP, CMPSC 448 Machine Learning or CSE 582 Natural Language Processing is the prerequisite. This course also assumes programming skills in Python and knowledge in linear algebra, calculus, basic probability and statistics.

<h2>Textbook</h2> 

The following textbooks are recommended for reading beyond papers listed in the course schedule. All of them are publicly available!
<ul> 
    <li><a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a> by Dan Jurafsky and James H. Martin.</li>  
    <li><a href="http://d2l.ai/">Dive into Deep Learning</a> by Aston Zhang, Zack C. Lipton, Mu Li, Alex J. Smola.</li>
    <li><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Natural Language Processing</a> by Jacob Eisenstein.</li>
    <li><a href="https://nlp.stanford.edu/fsnlp/">Foundations of Statistical Natural Language Processing</a> by Chris Manning and Hinrich Sch√ºtze.</li>
    <li><a href="http://www.cs.cmu.edu/~nasmith/LSP/">Linguistic Structure Prediction</a> by Noah A. Smith.</li>
    <li><a href="http://noiselab.ucsd.edu/ECE228/Murphy_Machine_Learning.pdf">Machine Learning: A Probabilistic Perspective</a> by Kevin Murphy.</li>
    <li><a href="http://www.morganclaypool.com/doi/abs/10.2200/S00493ED1V01Y201303HLT020">Linguistic Fundamentals for NLP</a> by Emily M. Bender.</li>
</ul>


<h2>Course Project</h2>

Project Format. This project aims to conduct original and independent research over NLP-related topics. Students can choose from several possible approaches:
<ol>
<li>Invent a new and important task in NLP and create a dataset for it, e.g., <a href="https://arxiv.org/pdf/2211.05041.pdf">MACSum: Controllable Summarization with Mixed Attributes</a>.</li>
<li>Create a new dataset for an existing NLP tasks, e.g., <a href="https://arxiv.org/pdf/1809.08887.pdf">Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task</a>.</li>
<li>Pick an existing NLP task and dataset and try to get good results, e.g., <a href="https://arxiv.org/pdf/2109.07589.pdf">CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning and check this awesome website Papers With Code: The latest in Machine Learning</a>.</li>
<li>Try to pick a NLP problem and dive deep into it with comprehensive analysis, e.g., <a href="https://arxiv.org/pdf/2206.04615.pdf">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</a>.</li>
</ol>

No matter which approach you take, we will hold your project to a high standard. Excellent projects will result in published papers in top-tier NLP/AI/ML conferences or journals (e.g., ACL, NAACL, EMNLP, AAAI, ICLR, NeurIPS, ICML, TACL, ...).
<br>
<br>
Group Policy. You can work on the course project in a group of 1-3 people. You are allowed to combine this project with your research projects or projects from other courses.
<br>
<br>
Deliverables.
<ol>
    <li>Project Proposal. Write a 3-page proposal that outlines your plan including what problem or task you want to address, what dataset(s) you want to work on, what metrics you need to employ, what baselines you would like to compare with. You should also cite a few relevant prior papers. Please use <a href="https://www.overleaf.com/read/nqhjfcfjvxpq">this overleaf templatet</a>.</li>
    <li>Final Report. Your final report should use our Latex template with at least 8-page plus references. Your report should begin with an abstract and introduction to clearly state the problem you want to solve and contributions you have made. It should also have a section on related work, a section on your methodology, a section on your experimental settings and results, and a section on conclusions. Please use <a href="https://www.overleaf.com/read/grpvkxgcdqdt">this overleaf template</a>.</li>
    <li>Code and Data. Please submit your data and runnable code with a detailed instruction. </li>
</ol>

<h2>Paper Presentation and Paper Review</h2>
In the second phase of this course, we will cover two paper presentations by students in one lecture.
<ul>
    <li>Please sign up for 1 paper presentation and 5 paper reviews by putting your names in [slot]. First come, first served. You have to review papers which are not presented on your presentation day.</li>
    <li>Please submit your presentation slides and paper reviews in Canvas -> Assignment by the end of day before the class, i.e., 11:59pm Sunday for Monday's classes and 11:59pm Tuesday for Wednesday's classes.</li>
    <li>Before your presentation, you are also required to meet me before the class: 1pm-2pm for Monday and 1pm-2pm for Wednesday.</li>
    <li>Each presentation will be 35mins (e.g., 25mins + 10mins Q&A).</li>
    <li>
    Please follow the <a href="https://2021.aclweb.org/downloads/Review_Form.pdf">review form from ACL</a> for paper review Becoming a good reviewer takes efforts and practice. If you haven't review an NLP paper before, here are guidelines for review a paper provided by community.
        <ul>
            <li><a href="https://aclrollingreview.org/reviewertutorial">How to review for ACL Rolling Review</a></li>
            <li><a href="https://aclrollingreview.org/reviewform">Review Form</a></li>
        </ul>
</li>
</ul>




<h2>Course Schedule</h2>
  <table class="table">
    <colgroup>
      <col style="width:10%">
      <col style="width:20%">
      <col style="width:40%">
      <col style="width:10%">
      <col style="width:10%">
    </colgroup>
    <thead>
    <tr style=" background-color: #fcf8e3">
      <th>Date</th>
      <th>Topic</th>
      <th>Material</th>
      <th>Event</th>
      <th>Due</th>
    </tr>
    </thead>
    <tbody>
        
    <tr>
        <td colspan="5" style="text-align: center;">Part 1: Lectures by Rui on NLP Foundations</td>
    </tr>
    <tr>
      <td><b><font color="red">Week 1</font></b><br>Monday Jan 9</td>
      <td>Introduction
        <br>
        [<a href="cse587/1_Introduction-to-NLP.pdf">slides</a>]
      </td>
      <td>
      </td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
      <td>Wednesday Jan 11</td>
      <td>Text Classification and Language Modeling
        <br>
        [<a href="cse587/2_Text-Classification-and-Language-Modeling.pdf">slides</a>]
      </td>
      <td>
      </td>
      <td></td>
      <td></td>
    </tr>

    <tr>
        <td><b><font color="red">Week 2</font><br></b>Monday Jan 16</td>
        <td>No Class</td>
        <td></td>
        <td></td>
        <td></td>
      </tr>
  
    <tr>
      <td>Wednesday Jan 18<br></td>
      <td>Neural Networks and Backpropagation
        <br>
        [<a href="cse587/3_Neural-Networks-and-Backpropagation.pdf">slides</a>]
      </td>
      <td>
      </td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
      <td><b><font color="red">Week 3</font></b><br>Monday Jan 23</td>
      <td>Neural Networks and Backpropagation
        <br>
        [<a href="cse587/3_Neural-Networks-and-Backpropagation.pdf">slides</a>]
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
      <td>Wednesday Jan 25</td>
      <td>Recurrent Neural Networks, Sequence-to-Sequence, and Attention
        <br>
        [<a href="cse587/4_RNNs-Sequence-to-Sequence-Attention.pdf">slides</a>]
      </td>
      <td>
      </td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
      <td><b><font color="red">Week 4</font></b><br>Monday Jan 30</td>
      <td>Final Project
        <br>
        [<a href="cse587/Final-Project.pdf">slides</a>]
      </td>
      <td>
      </td>
      <td>Assignment 1 <b><font color="green">Out</font></b></td>
      <td></td>
    </tr>
  
    <tr>
      <td>Wednesday Feb 1</td>
      <td>Transformers
        <br>
        [<a href="cse587/5_Transformers.pdf">slides</a>]
      </td>
      <td>
      </td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
      <td><b><font color="red">Week 5</font></b><br>Tue Feb 7</td>
      <td>Transformers
        <br>
        [<a href="cse587/5_Transformers.pdf">slides</a>]
      </td>
      <td>
      </td>
      <td></td>
      <td>Project Group Registration <b><font color="red">Due</font></b></td>
    </tr>
  
    <tr>
      <td>Wednesday Feb 8</td>
      <td>BERT and Pretraining
        <br>
        [<a href="cse587/6_BERT-and-Pretraining.pdf">slides</a>]
      </td>
      <td>
      </td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
      <td><b><font color="red">Week 6</font></b><br>Monday Feb 13</td>
      <td>BERT and Pretraining
        <br>
        [<a href="cse587/6_BERT-and-Pretraining.pdf">slides</a>]
      </td>
      <td>
      </td>
      <td></td>
      <td>Assignment 1 <b><font color="red">Due</font></b></td>
    </tr>
  
    <tr>
      <td>Wednesday Feb 15</td>
      <td>Prompt-based Methods
        <br>
        [<a href="cse587/7_Prompt-based-Methods.pdf">slides</a>]
      </td>
      <td>
      </td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
      <td><b><font color="red">Week 7</font></b><br>Monday Feb 20</td>
      <td>ChatGPT and Beyond<br>
        [<a href="cse587/8_ChatGPT-and-Beyond.pdf">slides</a>]
      </td>
      <td>
      </td>
      <td></td>
      <td>Project Proposal <b><font color="red">Due</font></b></td>
    </tr>
  
    <tr>
      <td>Wednesday Feb 22</td>
      <td>ChatGPT and Beyond<br>
        [<a href="cse587/8_ChatGPT-and-Beyond.pdf">slides</a>]
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
  
    <tr>
        <td colspan="5" style="text-align: center;">Part 2: Presentations by Students on NLP Frontiers</td>
    </tr>
    <tr>
      <td><b><font color="red">Week 8</font></b><br>Monday Feb 27</td>
      <td>GPT-2 and GPT-3</td>
      <td>
        Paper Presentations
        <ul>
          <li><a href="https://arxiv.org/abs/2208.07339">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a> <br> Presented by Songhe Wang [<a href="cse587/slides_Songhe_Wang.pdf">slides</a>]</li>
          <li><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a> <br> Presented by Mahsa Sheikhi [<a href="">slides</a>]</li>
          <li><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br> Presented by Shu Zhao [<a href="cse587/slides_Shu_Zhao.pdf">slides</a>]</li>
        </ul>
      </td>
      <td></td>
      <td></td>
    </tr>

    <tr>
        <td>Wednesday March 1</td>
        <td>In-Context Learning</td>
        <td>
          Paper Presentations
          <ul>
            <li><a href="https://arxiv.org/abs/2202.12837">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</a> <br> Presented by Renze Lou [<a href="">slides</a>]</li>
            <li><a href="https://arxiv.org/abs/2104.08786">Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</a> <br> Presented by Sarkar Snigdha Sarathi Das [<a href="">slides</a>]</li>
          </ul>
        </td>
        <td></td>
        <td></td>
    </tr>
  
    <tr>
        <td><b><font color="red">Week 9</font></b><br>Monday March 6</td>
        <td>Spring Break</td>
        <td></td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td>Wednesday March 8</td>
        <td>Spring Break</td>
        <td></td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td><b><font color="red">Week 10</font></b><br>Monday March 13</td>
        <td>Calibration</td>
        <td>
          Paper Presentations
          <ul>
            <li><a href="https://arxiv.org/abs/2102.09690">Calibrate Before Use: Improving Few-Shot Performance of Language Models</a> <br> Presented by Hamed Mahdavi [<a href="">slides</a>]</li>
            <li><a href="https://arxiv.org/abs/2207.05221">Language Models (Mostly) Know What They Know</a> <br> Presented by Yusen Zhang [<a href="">slides</a>]</li>
          </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td>Wednesday March 15</td>
        <td>Reasoning and Emergent Abilities</td>
        <td>
          Paper Presentations
          <ul>
            <li><a href="https://arxiv.org/abs/2201.11903">Chain of Thought Prompting Elicits Reasoning in Large Language Models</a> <br> Presented by Haoran Zhang [<a href="">slides</a>]</li>
            <li><a href="https://arxiv.org/abs/2206.07682">Emergent Abilities of Large Language Models</a> <br> Presented by Malyaban Bal [<a href="">slides</a>]</li>
          </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td><b><font color="red">Week 11</font></b><br>Monday March 20</td>
        <td>Diffusion Model</td>
        <td>
          Paper Presentations
          <ul>
            <li><a href="https://arxiv.org/abs/2205.11487">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</a> <br> Presented by Xinjie Li [<a href="">slides</a>]</li>
            <li><a href="https://arxiv.org/abs/2205.14217">Diffusion-LM Improves Controllable Text Generation</a> <br> Presented by Max Mehta [<a href="">slides</a>]</li>
          </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td>Wednesday March 22</td>
        <td>Multilingual Language Model</td>
        <td>
          Paper Presentations
          <ul>
            <li><a href="https://arxiv.org/abs/1901.07291">Cross-lingual Language Model Pretraining</a> <br> Presented by Samira Malek [<a href="">slides</a>]</li>
            <li><a href="https://arxiv.org/abs/2112.10668">Few-shot Learning with Multilingual Language Models</a> <br> Presented by Abdullah Al Ishtiaq [<a href="">slides</a>]</li>
          </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td><b><font color="red">Week 12</font></b><br>Monday March 27</td>
        <td>Multimodal Language Model</td>
        <td>
          Paper Presentations
          <ul>
            <li><a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a> <br> Presented by Liwei Che [<a href="">slides</a>]</li>
            <li><a href="https://arxiv.org/abs/2204.14198">Flamingo: a Visual Language Model for Few-Shot Learning</a> <br> Presented by Pouria Mahdavinia [<a href="">slides</a>]</li>
          </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td>Wednesday March 29</td>
        <td>Code Language Model</td>
        <td>
          Paper Presentations
          <ul>
            <li><a href="https://arxiv.org/abs/2107.03374">Evaluating Large Language Models Trained on Code</a> <br> Presented by Srinivas V [<a href="">slides</a>]</li>
            <li><a href="https://arxiv.org/abs/2204.05999">InCoder: A Generative Model for Code Infilling and Synthesis</a> <br> Presented by Levent Toksoz [<a href="">slides</a>]</li>
          </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td><b><font color="red">Week 13</font></b><br>Monday April 3</td>
        <td>Language Models for RL</td>
        <td>
          Paper Presentations
          <ul>
            <li><a href="https://arxiv.org/abs/2205.11558">Using Natural Language and Program Abstractions to Instill Human Inductive Biases in Machines</a> <br> Presented by Tianhui Li [<a href="">slides</a>]</li>
            <li><a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a> <br> Presented by Berk Atil [<a href="">slides</a>]</li>
          </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td>Wednesday April 5</td>
        <td>Knowledge</td>
        <td>
          Paper Presentations
          <ul>
            <li><a href="https://arxiv.org/abs/1909.01066">Language Models as Knowledge Bases?</a> <br> Presented by Huaisheng Zhu [<a href="">slides</a>]</li>
            <li><a href="https://arxiv.org/abs/2110.11309">Fast Model Editing at Scale</a> <br> Presented by Allen Gao [<a href="">slides</a>]</li>
          </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td><b><font color="red">Week 14</font></b><br>Monday April 10</td>
        <td>Reinforcement Learning from Human Feedback</td>
        <td>
          Paper Presentations
          <ul>
            <li><a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a> <br> Presented by Hangzhi Guo [<a href="">slides</a>]</li>
            <li><a href="https://arxiv.org/abs/2204.05862">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</a> <br> Presented by Keaton Kraiger [<a href="">slides</a>]</li>
          </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td>Wednesday April 12</td>
        <td>Task Generalization</td>
        <td>
          Paper Presentations
          <ul>
            <li><a href="https://arxiv.org/abs/2204.07705">Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks</a> <br> Presented by Xutong Wang [<a href="">slides</a>]</li>
            <li><a href="https://arxiv.org/abs/2110.08207">Multitask Prompted Training Enables Zero-Shot Task Generalization</a> <br> Presented by Alireza Sepehrinezhad [<a href="">slides</a>]</li>
          </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td><b><font color="red">Week 15</font></b><br>Monday April 17</td>
        <td>Evaluation</td>
        <td>
          Paper Presentations
          <ul>
            <li><a href="https://arxiv.org/abs/2211.09110">Holistic Evaluation of Language Models</a> <br> Presented by Mohammad Sabih [<a href="">slides</a>]</li>
            <li><a href="https://arxiv.org/abs/2211.09110">Holistic Evaluation of Language Models</a> <br> Presented by Alex Bi [<a href="">slides</a>]</li>
          </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td>Wednesday April 19</td>
        <td>Data</td>
        <td>
          Paper Presentations
          <ul>
            <li><a href="https://arxiv.org/abs/2104.08758">Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus</a> <br> Presented by Ruihao Pan [<a href="">slides</a>]</li>
            <li><a href="https://arxiv.org/abs/2107.06499">Deduplicating Training Data Makes Language Models Better</a> <br> Presented by Zhimeng Guo [<a href="">slides</a>]</li>
          </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td><b><font color="red">Week 16</font></b><br>Monday April 24</td>
        <td>Security and Privacy</td>
        <td>
          Paper Presentations
          <ul>
            <li><a href="https://arxiv.org/abs/2012.07805">Extracting Training Data from Large Language Models</a> <br> Presented by Hangfan Zhang [<a href="">slides</a>]</li>
            <li><a href="https://arxiv.org/abs/2202.07646">Quantifying Memorization Across Neural Language Models</a> <br> Presented by Vishnu Dasu [<a href="">slides</a>]</li>
          </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td>Wednesday April 26</td>
        <td>Social Impacts</td>
        <td>
          Paper Presentations
          <ul>
            <li><a href="https://arxiv.org/abs/2301.11305">DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature</a> <br> Presented by Medha Kumar [<a href="">slides</a>]</li>
            <li><a href="https://arxiv.org/abs/2301.10226">A Watermark for Large Language Models</a> <br> Presented by Arisha Rao [<a href="">slides</a>]</li>
          </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td>Friday May 5</td>
        <td></td>
        <td></td>
        <td></td>
        <td>Project Report, Code, and Data <b><font color="red">Due</font></b></td>
    </tr>

    </tbody>
  </table>
</div>

</body>

</html>
